{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dataset import get_2D_normalised, get_dimensionlly_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier:\n",
    "    def __init__(self, num_classes, input_size, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Constructor for the SoftmaxClassifier class.\n",
    "        :param num_classes: The number of classes in the classification problem.\n",
    "        :param input_size: The number of input features in the dataset.\n",
    "        :param learning_rate: The learning rate used in gradient descent.\n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = np.random.randn(input_size, num_classes) / np.sqrt(input_size)\n",
    "        self.b = np.zeros((1, num_classes))\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"\n",
    "        Softmax function for a given matrix of input logits.\n",
    "        :param Z: A matrix of input logits, with shape (m, num_classes).\n",
    "        :return: A matrix of output probabilities, with shape (m, num_classes).\n",
    "        \"\"\"\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for a given matrix of input features.\n",
    "        :param X: A matrix of input features, with shape (m, input_size).\n",
    "        :return: A vector of predicted class labels, with shape (m,).\n",
    "        \"\"\"\n",
    "        logits = np.dot(X, self.W) + self.b\n",
    "        probs = self.softmax(logits)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def train(self, X, y, num_epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the softmax classifier using gradient descent.\n",
    "        :param X: A matrix of input features, with shape (m, input_size).\n",
    "        :param y: A vector of class labels, with shape (m,).\n",
    "        :param num_epochs: The number of epochs to train for.\n",
    "        :param batch_size: The batch size to use for gradient descent.\n",
    "        \"\"\"\n",
    "        num_batches = int(np.ceil(len(X) / batch_size))\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(num_batches):\n",
    "                batch_start = i * batch_size\n",
    "                batch_end = (i + 1) * batch_size\n",
    "                X_batch = X[batch_start:batch_end]\n",
    "                y_batch = y[batch_start:batch_end]\n",
    "                dW, db = self.compute_gradients(X_batch, y_batch)\n",
    "                self.W -= self.learning_rate * dW\n",
    "                self.b -= self.learning_rate * db\n",
    "                \n",
    "    def compute_gradients(self, X_batch, y_batch):\n",
    "        logits = np.dot(X_batch, self.W) + self.b\n",
    "        probs = self.softmax(logits)\n",
    "        delta = probs\n",
    "        delta[range(len(X_batch)), y_batch] -= 1\n",
    "        dW = np.dot(X_batch.T, delta) / len(X_batch)\n",
    "        db = np.mean(delta, axis=0)\n",
    "        return dW, db\n",
    "                \n",
    "    def train_early(self, X, y, num_epochs=100, batch_size=32, learning_rate=0.01, reg_strength=0.0, early_stopping_patience=5, validation_frac=0.1):\n",
    "        \"\"\"\n",
    "        Train the SoftmaxClassifier on the given training data using mini-batch gradient descent with early stopping.\n",
    "\n",
    "        :param X: The training data, a Numpy array of shape (num_examples, input_size).\n",
    "        :param y: The training labels, a Numpy array of shape (num_examples,).\n",
    "        :param num_epochs: The number of epochs to train for.\n",
    "        :param batch_size: The mini-batch size to use for training.\n",
    "        :param learning_rate: The learning rate to use for gradient descent.\n",
    "        :param reg_strength: The L2 regularization strength to use for training.\n",
    "        :param early_stopping_patience: The number of epochs to wait without improvement in the validation accuracy before stopping training.\n",
    "        :param validation_frac: The fraction of the training data to use for validation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        num_examples = X.shape[0]\n",
    "        num_train = int(num_examples * (1 - validation_frac))\n",
    "        train_indices = np.arange(num_train)\n",
    "        val_indices = np.arange(num_train, num_examples)\n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "\n",
    "        # Initialize the weights and biases\n",
    "        self.W = np.random.randn(self.input_size, self.num_classes) / np.sqrt(self.input_size)\n",
    "        self.b = np.zeros(self.num_classes)\n",
    "\n",
    "        # Initialize the best validation accuracy and the number of epochs since the last improvement\n",
    "        best_val_acc = 0.0\n",
    "        epochs_since_last_improvement = 0\n",
    "\n",
    "        # Train the model for the specified number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the training data\n",
    "            np.random.shuffle(train_indices)\n",
    "\n",
    "            # Split the training data into mini-batches\n",
    "            for i in range(0, num_train, batch_size):\n",
    "                batch_indices = train_indices[i:i+batch_size]\n",
    "                X_batch, y_batch = X_train[batch_indices], y_train[batch_indices]\n",
    "\n",
    "                # Compute the gradients of the loss with respect to the weights and biases\n",
    "                dW, db = self.compute_gradients(X_batch, y_batch)\n",
    "\n",
    "                # Update the weights and biases using gradient descent\n",
    "                self.W -= learning_rate * (dW + reg_strength * self.W)\n",
    "                self.b -= learning_rate * db\n",
    "\n",
    "            # Compute the training and validation accuracies for this epoch\n",
    "            train_acc = np.mean(self.predict(X_train) == y_train)\n",
    "            val_acc = np.mean(self.predict(X_val) == y_val)\n",
    "\n",
    "            # Print the training and validation accuracies for this epoch\n",
    "            print(\"Epoch {}: training accuracy = {:.4f}, validation accuracy = {:.4f}\".format(epoch+1, train_acc, val_acc))\n",
    "\n",
    "            # Check if the validation accuracy has improved\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_since_last_improvement = 0\n",
    "            else:\n",
    "                epochs_since_last_improvement += 1\n",
    "\n",
    "            # Check if training should be stopped due to lack of improvement in the validation accuracy\n",
    "            if epochs_since_last_improvement >= early_stopping_patience:\n",
    "                print(\"Validation accuracy has not improved for {} epochs. Stopping training...\".format(epochs_since_last_improvement))\n",
    "                break\n",
    "\n",
    "        print(\"Training complete. Best validation accuracy = {:.4f}\".format(best_val_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = get_2D_normalised()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training accuracy = 0.2047, validation accuracy = 0.1996\n",
      "Epoch 2: training accuracy = 0.3327, validation accuracy = 0.3208\n",
      "Epoch 3: training accuracy = 0.3442, validation accuracy = 0.3366\n",
      "Epoch 4: training accuracy = 0.2966, validation accuracy = 0.2884\n",
      "Epoch 5: training accuracy = 0.3165, validation accuracy = 0.3018\n",
      "Epoch 6: training accuracy = 0.3630, validation accuracy = 0.3360\n",
      "Epoch 7: training accuracy = 0.3190, validation accuracy = 0.3092\n",
      "Epoch 8: training accuracy = 0.3399, validation accuracy = 0.3230\n",
      "Epoch 9: training accuracy = 0.3262, validation accuracy = 0.3162\n",
      "Epoch 10: training accuracy = 0.3474, validation accuracy = 0.3278\n",
      "Epoch 11: training accuracy = 0.3224, validation accuracy = 0.3100\n",
      "Epoch 12: training accuracy = 0.3597, validation accuracy = 0.3400\n",
      "Epoch 13: training accuracy = 0.4030, validation accuracy = 0.3784\n",
      "Epoch 14: training accuracy = 0.3848, validation accuracy = 0.3570\n",
      "Epoch 15: training accuracy = 0.3693, validation accuracy = 0.3516\n",
      "Epoch 16: training accuracy = 0.2981, validation accuracy = 0.2812\n",
      "Epoch 17: training accuracy = 0.3272, validation accuracy = 0.3150\n",
      "Epoch 18: training accuracy = 0.2947, validation accuracy = 0.2738\n",
      "Epoch 19: training accuracy = 0.3675, validation accuracy = 0.3366\n",
      "Epoch 20: training accuracy = 0.3094, validation accuracy = 0.2946\n",
      "Epoch 21: training accuracy = 0.3278, validation accuracy = 0.2960\n",
      "Epoch 22: training accuracy = 0.3963, validation accuracy = 0.3728\n",
      "Epoch 23: training accuracy = 0.3859, validation accuracy = 0.3482\n",
      "Validation accuracy has not improved for 10 epochs. Stopping training...\n",
      "Training complete. Best validation accuracy = 0.3784\n"
     ]
    }
   ],
   "source": [
    "softmax_clf = SoftmaxClassifier(num_classes=10, input_size=3072)\n",
    "softmax_clf.train_early(x_train, y_train, num_epochs=50, batch_size=32, early_stopping_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.3596\n"
     ]
    }
   ],
   "source": [
    "test_preds = softmax_clf.predict(x_test)\n",
    "\n",
    "accuracy = np.mean(test_preds == y_test)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_pca, y_train_pca), (x_test_pca, y_test_pca) = get_dimensionlly_reduced(components=1024, needed=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training accuracy = 0.3196, validation accuracy = 0.3036\n",
      "Epoch 2: training accuracy = 0.3592, validation accuracy = 0.3396\n",
      "Epoch 3: training accuracy = 0.3654, validation accuracy = 0.3370\n",
      "Epoch 4: training accuracy = 0.3213, validation accuracy = 0.2996\n",
      "Epoch 5: training accuracy = 0.3650, validation accuracy = 0.3300\n",
      "Epoch 6: training accuracy = 0.3599, validation accuracy = 0.3332\n",
      "Epoch 7: training accuracy = 0.3574, validation accuracy = 0.3264\n",
      "Epoch 8: training accuracy = 0.3580, validation accuracy = 0.3266\n",
      "Epoch 9: training accuracy = 0.3893, validation accuracy = 0.3540\n",
      "Epoch 10: training accuracy = 0.3618, validation accuracy = 0.3230\n",
      "Epoch 11: training accuracy = 0.3665, validation accuracy = 0.3262\n",
      "Epoch 12: training accuracy = 0.3214, validation accuracy = 0.3078\n",
      "Epoch 13: training accuracy = 0.3575, validation accuracy = 0.3244\n",
      "Epoch 14: training accuracy = 0.3696, validation accuracy = 0.3188\n",
      "Epoch 15: training accuracy = 0.3575, validation accuracy = 0.3242\n",
      "Epoch 16: training accuracy = 0.3482, validation accuracy = 0.3088\n",
      "Epoch 17: training accuracy = 0.3761, validation accuracy = 0.3368\n",
      "Epoch 18: training accuracy = 0.3390, validation accuracy = 0.2948\n",
      "Epoch 19: training accuracy = 0.3880, validation accuracy = 0.3468\n",
      "Validation accuracy has not improved for 10 epochs. Stopping training...\n",
      "Training complete. Best validation accuracy = 0.3540\n"
     ]
    }
   ],
   "source": [
    "softmax_pca = SoftmaxClassifier(num_classes=10, input_size=1024)\n",
    "softmax_pca.train_early(x_train_pca, y_train_pca, num_epochs=50, batch_size=32, early_stopping_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.3427\n"
     ]
    }
   ],
   "source": [
    "test_preds_pca = softmax_pca.predict(x_test_pca)\n",
    "\n",
    "accuracy_pca = np.mean(test_preds_pca == y_test_pca)\n",
    "print(\"Test accuracy:\", accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
